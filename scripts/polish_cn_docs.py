#!/usr/bin/env python3
from __future__ import annotations

import argparse
import re
from pathlib import Path
from typing import Iterable, List, Tuple

SUPPORTED_EXTENSIONS = {".md", ".txt", ".rst", ".adoc"}
EXCLUDED_DIRS = {
    ".git",
    "node_modules",
    "venv",
    ".venv",
    "__pycache__",
    ".pytest_cache",
    "dist",
    "build",
    ".mypy_cache",
    ".tox",
}

_CJK_RE = re.compile(r"[\u4e00-\u9fff]")
_INLINE_CODE_SPLIT_RE = re.compile(r"(`[^`]*`)")
_MD_INLINE_LINK_RE = re.compile(r"\[([^\]\n]+)\]\(([^)\n]+)\)")
_MD_REF_DEF_RE = re.compile(r"^\s{0,3}\[[^\]]+\]:\s+\S+")
_MD_HEADING_RE = re.compile(r"^(#{1,6})(\s+)(.+)$")
_MD_BOLD_RE = re.compile(r"\*\*([^*]+)\*\*")


def find_documents(root_dir: Path, extensions: set[str] | None = None) -> List[Path]:
    if extensions is None:
        extensions = SUPPORTED_EXTENSIONS

    docs: List[Path] = []
    for file_path in root_dir.rglob("*"):
        if not file_path.is_file():
            continue
        if any(excluded in file_path.parts for excluded in EXCLUDED_DIRS):
            continue
        if file_path.suffix.lower() in extensions:
            docs.append(file_path)
    return sorted(docs)


def split_yaml_frontmatter(lines: List[str]) -> Tuple[List[str], List[str]]:
    if not lines or lines[0].strip() != "---":
        return [], lines
    for i in range(1, min(len(lines), 200)):
        if lines[i].strip() == "---":
            return lines[: i + 1], lines[i + 1 :]
    return [], lines


def has_cjk(text: str) -> bool:
    return bool(_CJK_RE.search(text))


def normalize_spacing(text: str) -> str:
    # Insert spaces between CJK and ASCII letters/digits where they touch.
    text = re.sub(r"([\u4e00-\u9fff])([A-Za-z0-9])", r"\1 \2", text)
    text = re.sub(r"([A-Za-z0-9])([\u4e00-\u9fff])", r"\1 \2", text)
    return text


def normalize_punctuation(text: str) -> str:
    if not has_cjk(text):
        return text
    # Only normalize a few safe ASCII punctuations inside Chinese prose.
    text = re.sub(r"([\u4e00-\u9fff]),", r"\1ï¼Œ", text)
    text = re.sub(r",([\u4e00-\u9fff])", r"ï¼Œ\1", text)
    text = re.sub(r"([\u4e00-\u9fff])\.", r"\1ã€‚", text)
    text = re.sub(r"([\u4e00-\u9fff]):", r"\1ï¼š", text)
    return text


def build_replacements() -> List[tuple[re.Pattern[str], str]]:
    # Order matters: do specific product-name fixes before generic ones.
    rules: List[tuple[str, str]] = [
        (r"å…‹æ´›å¾·ä»£ç ", "Claude Code"),
        (r"å…‹åŠ³å¾·ä»£ç ", "Claude Code"),
        (r"å…‹æ´›å¾·\s*Code", "Claude Code"),
        (r"å…‹åŠ³å¾·\s*Code", "Claude Code"),
        (r"å…‹æ´›å¾·ç‰¹å·¥SDK", "Claude Agent SDK"),
        (r"å…‹åŠ³å¾·ç‰¹å·¥SDK", "Claude Agent SDK"),
        (r"ç‰¹å·¥SDK", "Agent SDK"),
        (r"ä»£ç†SDK", "Agent SDK"),
        (r"è™å…‹ä»¬", "Hooks"),
        (r"è™å…‹", "Hook"),
        (r"é‡æ–°æµè¡Œ", "RepoPrompt"),
        (r"å†æµè¡Œ", "RepoPrompt"),
        (r"äººæ‰ä¿¡ä»»ä¼šè®®", "Braintrust ä¼šè¯"),
        (r"äººæ‰ä¿¡ä»»ä¼š", "Braintrust ä¼šè¯"),
        (r"äººæ‰ä¿¡ä»»", "Braintrust"),
        (r"è„‘ä¿¡æ‰˜", "Braintrust"),
        (r"è„‘ä¿¡ä¼š", "Braintrust"),
        (r"è„‘ä¿¡", "Braintrust"),
        (r"Artiffact", "Artifact"),
        (r"è‰ºæœ¯ç´¢å¼•", "åˆ¶å“ç´¢å¼•ï¼ˆArtifact Indexï¼‰"),
        (r"ç†ç”±å†å²", "æ¨ç†å†å²"),
        (r"éšœç¢å°„å‡»", "æ•…éšœæ’é™¤"),
        (r"æ ‡è¯­æ•ˆç‡é«˜çš„", "é«˜ token æ•ˆç‡çš„"),
        (r"å…¨çƒå®‰è£…", "å…¨å±€å®‰è£…"),
        (r"ä½œæ›²æ ‡ç­¾", "Composer æ ‡ç­¾"),
        (r"æ´»æ€§æ ‡ç­¾", "æ´»åŠ¨æ ‡ç­¾"),
        (r"ç‰¹å·¥ä»¬", "ä»£ç†"),
        (r"ç‰¹å·¥äººå‘˜", "ä»£ç†"),
        (r"ç‰¹å·¥", "ä»£ç†"),
        (r"å…‹æ´›å¾·", "Claude"),
        (r"å…‹åŠ³å¾·", "Claude"),
    ]

    compiled: List[tuple[re.Pattern[str], str]] = []
    for pat, repl in rules:
        compiled.append((re.compile(pat), repl))
    return compiled


def apply_replacements(text: str, replacements: List[tuple[re.Pattern[str], str]]) -> str:
    for pattern, repl in replacements:
        text = pattern.sub(repl, text)
    return text


def polish_text_segment(text: str, replacements: List[tuple[re.Pattern[str], str]]) -> str:
    text = apply_replacements(text, replacements)
    text = normalize_spacing(text)
    text = normalize_punctuation(text)
    return text


def polish_markdown_line(line: str, replacements: List[tuple[re.Pattern[str], str]]) -> str:
    if _MD_REF_DEF_RE.match(line):
        return line
    # Keep injected HTML anchors as-is.
    if line.lstrip().startswith("<a ") and "id=" in line:
        return line

    # Headings: prefer "æ¶æ„" over "å»ºç­‘" in architecture contexts.
    m = _MD_HEADING_RE.match(line)
    if m:
        heading = m.group(3)
        if heading.startswith("å»ºç­‘"):
            heading = "æ¶æ„" + heading[len("å»ºç­‘") :]
            line = f"{m.group(1)}{m.group(2)}{heading}"

    # Common labels
    if line.startswith("è¡ŒåŠ¨:"):
        line = "æ“ä½œï¼š" + line[len("è¡ŒåŠ¨:") :]

    def polish_outside_inline_code(text: str) -> str:
        parts = _INLINE_CODE_SPLIT_RE.split(text)
        out: List[str] = []
        for part in parts:
            if not part:
                continue
            if part.startswith("`") and part.endswith("`"):
                out.append(part)
            else:
                segment = polish_text_segment(part, replacements)

                def normalize_bold(mb: re.Match[str]) -> str:
                    inner = mb.group(1).strip()
                    inner = normalize_punctuation(inner)
                    return f"**{inner}**"

                segment = _MD_BOLD_RE.sub(normalize_bold, segment)
                out.append(segment)
        return "".join(out)

    out: List[str] = []
    pos = 0
    for m in _MD_INLINE_LINK_RE.finditer(line):
        before = line[pos : m.start()]
        if before:
            out.append(polish_outside_inline_code(before))
        label = m.group(1)
        dest = m.group(2)
        out.append(f"[{polish_outside_inline_code(label)}]({dest})")
        pos = m.end()
    tail = line[pos:]
    if tail:
        out.append(polish_outside_inline_code(tail))
    return "".join(out)


def iter_polished_lines(path: Path, replacements: List[tuple[re.Pattern[str], str]]) -> Iterable[str]:
    raw = path.read_text(encoding="utf-8", errors="ignore")
    lines = raw.splitlines()

    frontmatter: List[str] = []
    rest = lines
    if path.suffix.lower() == ".md":
        frontmatter, rest = split_yaml_frontmatter(lines)

    for line in frontmatter:
        yield line

    in_fence = False
    fence_marker = ""

    for line in rest:
        stripped = line.strip()

        if stripped.startswith("```") or stripped.startswith("~~~"):
            marker = stripped[:3]
            if not in_fence:
                in_fence = True
                fence_marker = marker
            else:
                if fence_marker == marker:
                    in_fence = False
                    fence_marker = ""
            yield line
            continue

        if in_fence:
            yield line
            continue

        # indented code block
        if line.startswith("\t") or line.startswith("    "):
            yield line
            continue

        yield polish_markdown_line(line, replacements)

    if raw.endswith("\n"):
        # Preserve trailing newline for callers that join with '\n'
        yield ""


def main() -> None:
    parser = argparse.ArgumentParser(description="å¯¹å·²ç¿»è¯‘ä¸­æ–‡æ–‡æ¡£åšæœ¯è¯­ç»Ÿä¸€/è½»é‡æ¶¦è‰²ï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰")
    parser.add_argument("--dir", default=".", help="æ ¹ç›®å½•ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="ä»…é¢„è§ˆï¼Œä¸å†™å…¥")
    parser.add_argument("--yes", action="store_true", help="è·³è¿‡ç¡®è®¤")
    args = parser.parse_args()

    root = Path(args.dir)
    docs = find_documents(root)

    if args.dry_run:
        print("ğŸ” é¢„è§ˆæ¨¡å¼ - å°†å¤„ç†ä»¥ä¸‹æ–‡ä»¶ï¼š")
        for p in docs:
            print(f"  - {p}")
        return

    if not args.yes:
        resp = input(f"âš ï¸  å°†å¯¹ {len(docs)} ä¸ªæ–‡ä»¶åšæœ¯è¯­ç»Ÿä¸€å¹¶è¦†ç›–åŸæ–‡ä»¶ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ[y/N]: ")
        if resp.lower() != "y":
            print("âŒ å·²å–æ¶ˆ")
            return

    replacements = build_replacements()

    changed = 0
    for i, path in enumerate(docs, 1):
        # Skip LICENSE: keep legal English untouched (it has its own bilingual section already).
        if path.name == "LICENSE":
            continue

        original = path.read_text(encoding="utf-8", errors="ignore")
        polished = "\n".join(iter_polished_lines(path, replacements))
        if polished == original:
            continue
        path.write_text(polished, encoding="utf-8")
        changed += 1
        print(f"[{i}/{len(docs)}] âœ“ {path}")

    print(f"âœ“ å®Œæˆï¼šä¿®æ”¹ {changed} ä¸ªæ–‡ä»¶")


if __name__ == "__main__":
    main()
